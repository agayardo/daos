hosts:
  test_servers: 2
  test_clients: 1
timeout: 120
server_config:
  name: daos_server
  engines_per_host: 2
  engines:
    0:
      targets: 4
      pinned_numa_node: 0
      nr_xs_helpers: 1
      fabric_iface: ib0
      fabric_iface_port: 31317
      log_file: daos_engine_0.log
      storage:
        0:
          class: dcpm
          scm_list: ["/dev/pmem0"]
          scm_mount: /mnt/daos0
        1:
          class: nvme
          bdev_list: ["aaaa:aa:aa.a"]
    1:
      targets: 4
      pinned_numa_node: 1
      nr_xs_helpers: 1
      fabric_iface: ib1
      fabric_iface_port: 32317
      log_file: daos_engine_1.log
      storage:
        0:
          class: dcpm
          scm_list: ["/dev/pmem1"]
          scm_mount: /mnt/daos1
        1:
          class: nvme
          bdev_list: ["bbbb:bb:bb.b"]
pool_scm:
  scm_size: 1G
  nvme_size: 0
  control_method: dmg
pool_scm_nvme:
  size: 80%
  control_method: dmg
container:
  type: POSIX
  control_method: daos
  properties: rd_fac:0
  oclass: SX
ior:
  api: DFS
  # 1MiB
  transfer_size: 1048576
  # 128MiB
  block_size: 134217728
  flags: "-v -w -k"
  test_file: "/testFile"
  dfs_oclass: SX
  dfs_chunk: 1MiB
  dfs_destroy: false
  env_vars:
    - D_LOG_MASK=INFO
mpirun:
  bind_to: socket

scm_metric_thresholds:
  # The size of the SCM used is not directly managed by DAOS, but through the PMDK library.
  # Thus, the size of the SCM used mertric is not perfectly accurate as for NVME (thanks to VOS).
  # To cope with this issue an arbitrary threshold of 10% is used .
  data_size_percent: 10
  # Maximal metadata size is empirically adjusted to 1MiB
  metadata_max_size: 1048576
